# 自动微分(Automatic Differentiation, Autograd）)
自动微分是深度学习框架的核心技术，用于高效计算复合函数的梯度。其核心思想是通过`链式法则`将复杂函数的导数分解为基本操作的局部导数，并`反向累积梯度(backpropagate)`。

## 与数值微分、符号微分的区别：
- 数值微分：通过有限差分近似，计算量大且精度低。
- 符号微分：直接对表达式求解析导数，但对复杂函数易导致表达式膨胀。
- 自动微分：基于`计算图(computational graph)`动态分解函数，结合链式法则高效计算梯度。

### 计算图(computational graph)：
自动微分将计算过程表示为`有向无环图（DAG）`，记录前向传播的操作步骤。反向传播时，从输出到输入逐层计算梯度。

## 通过PyTorch调用自动微分
PyTorch通过动态计算图实现自动微分，主要包括以下步骤：
- 启用梯度追踪：将张量的` requires_grad `设为` True`。
- 执行前向计算：框架自动记录操作，构建计算图。
- 反向传播：调用` .backward() `自动计算梯度。
- 获取梯度：通过` .grad `属性访问梯度。

